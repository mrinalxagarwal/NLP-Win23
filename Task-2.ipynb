{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196002d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc64829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c1a6d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac91109",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac69b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 6, 'of': 5, 'and': 5, 'language': 3, 'is': 2, 'computer': 2, 'computers': 2, 'in': 2, 'to': 2, 'The': 2, ...})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'\n",
    "fd = nltk.FreqDist(text1.split())\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa07f1",
   "metadata": {},
   "source": [
    "# Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877f3ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'with': 1, 'goal': 1, 'then': 1, 'well': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist as confd\n",
    "cfd = confd((len(word),word) for word in text1.split())\n",
    "cfd[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa769b",
   "metadata": {},
   "source": [
    "# Task: Determine FD and CFD of any one of the Presidential Inaugural addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550e47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f54f3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 106, '.': 89, 'and': 70, 'the': 65, 'of': 48, 'our': 47, 'will': 43, 'to': 37, 'We': 26, 'we': 24, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = inaugural.words(fileids='2017-Trump.txt')\n",
    "fd = nltk.FreqDist(text1)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bff05f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'America': 20, 'country': 9, 'whether': 3, 'foreign': 3, 'nations': 3, 'borders': 3, 'another': 2, 'Capital': 2, 'belongs': 2, 'success': 2, ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist as confd\n",
    "cfd = confd((len(word),word) for word in text1)\n",
    "cfd[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629fb60",
   "metadata": {},
   "source": [
    "# Chinese Segmentation using Jieba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0d014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in c:\\users\\agarw\\anaconda3\\lib\\site-packages (0.42.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0b43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\agarw\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.128 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "史密斯 密斯 是 王明 的 朋友\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg = jieba.cut(\"史密斯是王明的朋友\", cut_all=True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b9518",
   "metadata": {},
   "source": [
    "# Basic Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fca4e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Become', 'an', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization of a sentence\n",
    "import nltk\n",
    "sent = \"Become an expert in NLP\"\n",
    "words = nltk.word_tokenize(sent)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8accd3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Onika Tanya Maraj-Petty (née Maraj; born December 8, 1982), known professionally as Nicki Minaj, is a Trinidadian-born rapper, singer, and songwriter based in the United States.',\n",
       " 'She is known for her her musical versatility, animated flow in her rapping, alter egos and accents.',\n",
       " 'Minaj is regarded as being the most influential female rapper of her generation and has been called the greatest female rapper of the 21st century.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "\n",
    "# Incase we are taking more than one paragraphs, we can use a for loop\n",
    "# for text in texts: (following code) -- where texts are the paragraphs\n",
    "\n",
    "text = \"Onika Tanya Maraj-Petty (née Maraj; born December 8, 1982), known professionally as Nicki Minaj, is a Trinidadian-born rapper, singer, and songwriter based in the United States. She is known for her her musical versatility, animated flow in her rapping, alter egos and accents. Minaj is regarded as being the most influential female rapper of her generation and has been called the greatest female rapper of the 21st century.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "918a0d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Onika', 'Tanya', 'Maraj-Petty', '(', 'née', 'Maraj', ';', 'born', 'December', '8', ',', '1982', ')', ',', 'known', 'professionally', 'as', 'Nicki', 'Minaj', ',', 'is', 'a', 'Trinidadian-born', 'rapper', ',', 'singer', ',', 'and', 'songwriter', 'based', 'in', 'the', 'United', 'States', '.']\n",
      "['She', 'is', 'known', 'for', 'her', 'her', 'musical', 'versatility', ',', 'animated', 'flow', 'in', 'her', 'rapping', ',', 'alter', 'egos', 'and', 'accents', '.']\n",
      "['Minaj', 'is', 'regarded', 'as', 'being', 'the', 'most', 'influential', 'female', 'rapper', 'of', 'her', 'generation', 'and', 'has', 'been', 'called', 'the', 'greatest', 'female', 'rapper', 'of', 'the', '21st', 'century', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization of all the sentences found in above paragraph\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77f566b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Minaj', 'NNP'), ('is', 'VBZ'), ('regarded', 'VBN'), ('as', 'IN'), ('being', 'VBG'), ('the', 'DT'), ('most', 'RBS'), ('influential', 'JJ'), ('female', 'JJ'), ('rapper', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('generation', 'NN'), ('and', 'CC'), ('has', 'VBZ'), ('been', 'VBN'), ('called', 'VBN'), ('the', 'DT'), ('greatest', 'JJS'), ('female', 'JJ'), ('rapper', 'NN'), ('of', 'IN'), ('the', 'DT'), ('21st', 'JJ'), ('century', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(words)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf36f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
